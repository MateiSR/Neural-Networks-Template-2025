{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117046,"databundleVersionId":13980102,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"start_time = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:13.027064Z","iopub.execute_input":"2025-11-11T08:14:13.027491Z","iopub.status.idle":"2025-11-11T08:14:13.032748Z","shell.execute_reply.started":"2025-11-11T08:14:13.027458Z","shell.execute_reply":"2025-11-11T08:14:13.030908Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_file = \"/kaggle/input/fii-nn-2025-homework-3/extended_mnist_train.pkl\"\ntest_file = \"/kaggle/input/fii-nn-2025-homework-3/extended_mnist_test.pkl\"\n\nwith open(train_file, \"rb\") as fp:\n    train = pickle.load(fp)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:13.035441Z","iopub.execute_input":"2025-11-11T08:14:13.035875Z","iopub.status.idle":"2025-11-11T08:14:13.831224Z","shell.execute_reply.started":"2025-11-11T08:14:13.035838Z","shell.execute_reply":"2025-11-11T08:14:13.830024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(test_file, \"rb\") as fp:\n    test = pickle.load(fp)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:13.035441Z","iopub.execute_input":"2025-11-11T08:14:13.035875Z","iopub.status.idle":"2025-11-11T08:14:13.831224Z","shell.execute_reply.started":"2025-11-11T08:14:13.035838Z","shell.execute_reply":"2025-11-11T08:14:13.830024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = []\ntrain_labels = []\nfor image, label in train:\n    train_data.append(image.flatten())\n    train_labels.append(label)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:13.832367Z","iopub.execute_input":"2025-11-11T08:14:13.832938Z","iopub.status.idle":"2025-11-11T08:14:13.987371Z","shell.execute_reply.started":"2025-11-11T08:14:13.832900Z","shell.execute_reply":"2025-11-11T08:14:13.986057Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"test_data = []\nfor image, label in test:\n    test_data.append(image.flatten())\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:13.988884Z","iopub.execute_input":"2025-11-11T08:14:13.989215Z","iopub.status.idle":"2025-11-11T08:14:14.025511Z","shell.execute_reply.started":"2025-11-11T08:14:13.989179Z","shell.execute_reply":"2025-11-11T08:14:14.024474Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# convert to numpy arrays\nX_train = np.array(train_data)\ny_train = np.array(train_labels)\nX_test = np.array(test_data)\n\n# normalize data (255 values for 0-255 black-white images)\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n# shuffle data\nshuffle_indexes = np.random.permutation(len(X_train))\nX_train = X_train[shuffle_indexes]\ny_train = y_train[shuffle_indexes]\n\n# train and validation data split\nnum_samples = X_train.shape[0]\nsplit_index = int(num_samples * 0.9)\nX_val = X_train[split_index:]\ny_val = y_train[split_index:]\nX_train = X_train[:split_index]\ny_train = y_train[:split_index]\n\n# standarizare (z-score normalization)\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0) + 1e-9\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\ndef one_hot(y, num_classes):\n    # one-hot, meaning:\n    # for digit n, we create a vector of size num_classes\n    # all values are 0, except the nth position, which is 1\n    one_hot_y = np.zeros((y.shape[0], num_classes))\n    for row, col in enumerate(y):\n        one_hot_y[row, col] = 1 # row = sample, col = digit 0-9\n    return one_hot_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:14.026813Z","iopub.execute_input":"2025-11-11T08:14:14.027081Z","iopub.status.idle":"2025-11-11T08:14:15.605715Z","shell.execute_reply.started":"2025-11-11T08:14:14.027058Z","shell.execute_reply":"2025-11-11T08:14:15.604870Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class MLPerceptron:\n    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate=0.5):\n        # INPUT TO HIDDEN\n        # weight initialized with small random values\n        self.W1 = np.random.randn(n_inputs, n_hidden) * np.sqrt(2.0 / n_inputs) #  He initialization\n        # bias initialized with 0\n        self.b1 = np.zeros(n_hidden)\n        # HIDDEN TO OUTPUT\n        self.W2 = np.random.randn(n_hidden, n_outputs) * np.sqrt(2.0 / n_hidden) #  He initialization\n        self.b2 = np.zeros(n_outputs)\n        # learning rate\n        self.lr = learning_rate\n\n    def _relu(self, z):\n        # rectified linear unit\n        return np.maximum(0, z)\n\n    def _relu_derivative(self, z):\n        # 1 if z > 0, 0 if z <= 0\n        return z > 0\n\n    def _softmax(self, z):\n        # softmax activation formula (numerically stable version)\n        # subtract max for numerical stability to prevent overflow in predict()\n        z_2 = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_2)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        ### forward step\n        # basically calculate z1 from a0(X) @ w1 + b1, through activation function\n        # a1 = reLU(z1). repeat again from the 2nd layer (hidden to output)\n        # get z2, then y_predicted = a2 (if more layers existed)\n\n        # LAYER 1\n        # @ = matrix multiplication\n        # z = XW + b\n        z1 = X @ self.W1 + self.b1 # X <=> a0 - previous action\n        # a = activation(z)\n        a1 = self._relu(z1)\n        # LAYER 2\n        z2 = a1 @ self.W2 + self.b2 # a1 - previous action before z2\n        y_predicted = self._softmax(z2)\n        return z1, a1, z2, y_predicted\n\n    def backward(self, X_batch, y_batch, z1, a1, z2, y_pred):\n        ### backward step\n        # C = cost function (cross-entropy loss)\n        num_samples_batch = X_batch.shape[0]\n\n        ### LAYER 2 (OUTPUT)\n        # error of output layer\n        # derivata compusa din softmax+cross-entropy loss\n        error_term_2 = y_batch - y_pred # (T-Y) = -dC/dz2\n        # how sensitive cost function is to changes in weight w2\n        # grad_W2 prin chain rule:\n        # z2 = a1 @ W2, deci dz2/dW2 produce a1^T\n        # dC/dW2 = dz2/dW2 Ã— dLoss/dz2 = a1^T @ error_term_2\n        grad_W2 = a1.T @ error_term_2\n        # sum, bias added to each sample\n        grad_b2 = np.sum(error_term_2, axis = 0)\n\n        ### LAYER 1 (HIDDEN) \n        # error of hidden layer\n        # how sensitive loss is to z1\n        # propagate error to hidden layer & apply chain rule with relu\n        # dC/da1 = (Y-T) @ W2.T\n        # error_term_1 = dC/dz1 = dc/da1 * da1/dz1 = ((Y-T) @ W2.T) * relu_derivative(z1)\n        error_term_1 = (error_term_2 @ self.W2.T) * self._relu_derivative(z1)\n        # grad_W1 prin chain rule:\n        # C -> z1 -> W1 => dC/dW1 = dC/dz1 * dz1/dW1\n        # z1 = X @ W1 => dz1/dW1 = X.T\n        # => grad_W1 ( = dC/dW1) = dz1/dW1 @ error_term 1 = X.T @ error_term_1\n        grad_W1 = X_batch.T @ error_term_1\n        grad_b1 = np.sum(error_term_1, axis = 0)\n\n        # gradient ascent (error term 1 negated) => gradient descent >\n        # update weights and biases\n        self.W1 += self.lr * (grad_W1 / num_samples_batch)\n        self.b1 += self.lr * (grad_b1 / num_samples_batch)\n        self.W2 += self.lr * (grad_W2 / num_samples_batch)\n        self.b2 += self.lr * (grad_b2 / num_samples_batch)\n    \n    def fit(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=60):\n        # number of samples\n        num_samples_train = X_train.shape[0]\n        num_samples_val = X_val.shape[0]\n        num_classes = self.b2.shape[0] # always 10 (digits 0 to 9)\n        \n        y_train_one_hot = one_hot(y_train, num_classes)\n        y_val_one_hot = one_hot(y_val, num_classes)\n\n        # learning rate scheduler data initialization\n        # https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n        best_val_loss = float('inf')\n        patience_cnt = 0\n        patience = 15\n        decay_factor = 0.25\n        min_lr = 1e-4\n        # early stopping based on accuracy improvement\n        best_val_acc = 0\n        early_stop_patience = 30 # Stop after no improvements for 30 epochs\n        no_improve_epochs = 0\n        \n        print(\"TRAINING: START!\")\n        # pass through the dataset EPOCHS times\n        for epoch in range(epochs):\n            \n            # shuffle training data\n            perm = np.random.permutation(num_samples_train)\n            X_shuffled = X_train[perm]\n            y_shuffled = y_train_one_hot[perm]\n\n            # iterate over batches\n            for i in range(0, num_samples_train, batch_size):\n                X_batch = X_shuffled[i:i+batch_size]\n                y_batch = y_shuffled[i:i+batch_size]\n\n                # forward pass\n                z1, a1, z2, y_pred = self.forward(X_batch)\n                # backward pass\n                self.backward(X_batch, y_batch, z1, a1, z2, y_pred)\n\n            # learning rate scheduler\n            _, _, _, y_pred_probs_val = self.forward(X_val)\n            val_loss = -np.sum(y_val_one_hot * np.log(y_pred_probs_val + 1e-9)) / num_samples_val\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_cnt = 0\n            else:\n                patience_cnt += 1\n                if patience_cnt > patience:\n                    initial_lr = self.lr\n                    new_lr = self.lr * decay_factor\n                    if new_lr >= min_lr:\n                        self.lr = new_lr\n                        patience_cnt = 0\n                        print(f\"(!!!) [EPOCH {epoch}] Learning rate reduced from {initial_lr:.5f} to {new_lr:.5f}\")\n                    else:\n                        print(f\"Learning rate already at minimum of {min_lr}, not updating.\")\n                    \n            # early stop\n            y_pred_classes_val = np.argmax(y_pred_probs_val, axis=1)\n            val_acc = np.mean(y_pred_classes_val == y_val)\n            if round(val_acc, 5) > best_val_acc:\n                best_val_acc = round(val_acc, 5)\n                no_improve_epochs = 0\n            else:\n                no_improve_epochs += 1\n            \n            if (epoch + 1) % 10 == 0 or epoch == 0:\n                # TRAIN DATA\n                _, _, _, y_pred_probs_train = self.forward(X_train)\n                # cross-entropy loss\n                train_loss = -np.sum(y_train_one_hot * np.log(y_pred_probs_train + 1e-9)) / num_samples_train\n                # calculate accuracy: compare predicted classes to true classes\n                y_pred_classes_train = np.argmax(y_pred_probs_train, axis=1)\n                train_acc = np.mean(y_pred_classes_train == y_train)\n                # VALIDATION DATA\n                # y_pred_probs_val ^^^\n                # cross-entropy loss\n                # val_loss ^^^\n                # calculate accuracy: compare predicted classes to true classes\n                # calculated above ^^^\n                \n                print(f\"--- Epoch {epoch + 1}/{epochs} ---\")\n                print(f\"  Train loss: {train_loss:.5f}, train acc: {train_acc:.5f}\")\n                print(f\"  Val   loss: {val_loss:.5f}, val   acc: {val_acc:.5f}\")\n\n            # early stop if no improvement\n            if no_improve_epochs >= early_stop_patience:\n                print(f\"(!!!) Early stop! No validation accuracy improvement for {no_improve_epochs} epochs!\")\n                break\n\n        print(\"TRAINING: COMPLETE!\")\n\n    def predict(self, X):\n        # get predicted probabilities (returns a matrix of 10 columns,\n        # one for each digit 0-9) and len(X) rows\n        _, _, _, y_probabilities = self.forward(X)\n        # return the label with highest probability for each row (sample)\n        # np.argmax - index cu val max (predicted)\n        return np.argmax(y_probabilities, axis=1)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:15.606648Z","iopub.execute_input":"2025-11-11T08:14:15.606927Z","iopub.status.idle":"2025-11-11T08:14:15.638956Z","shell.execute_reply.started":"2025-11-11T08:14:15.606899Z","shell.execute_reply":"2025-11-11T08:14:15.637927Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"n_inputs = 784 # 28x28 images flattened\nn_hidden = 100\nn_outputs = 10 # digits 0-9\nlearning_rate = 0.1\nepochs = 300\nbatch_size= 60\n\nmodel = MLPerceptron(n_inputs=n_inputs, n_hidden=n_hidden, n_outputs=n_outputs, learning_rate=learning_rate)\n# data, labels, no. epochs\nmodel.fit(X_train, y_train, X_val, y_val, epochs=epochs, batch_size=batch_size)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:14:15.640071Z","iopub.execute_input":"2025-11-11T08:14:15.640386Z","iopub.status.idle":"2025-11-11T08:15:26.769049Z","shell.execute_reply.started":"2025-11-11T08:14:15.640357Z","shell.execute_reply":"2025-11-11T08:15:26.768055Z"}},"outputs":[{"name":"stdout","text":"TRAINING: START!\n--- Epoch 1/300 ---\n  Train loss: 0.14509, train acc: 0.95961\n  Val   loss: 0.20448, val   acc: 0.95017\n--- Epoch 10/300 ---\n  Train loss: 0.01332, train acc: 0.99798\n  Val   loss: 0.13673, val   acc: 0.97117\n--- Epoch 20/300 ---\n  Train loss: 0.00310, train acc: 0.99998\n  Val   loss: 0.14703, val   acc: 0.97300\n(!!!) [EPOCH 23] Learning rate reduced from 0.10000 to 0.02500\n--- Epoch 30/300 ---\n  Train loss: 0.00195, train acc: 1.00000\n  Val   loss: 0.15097, val   acc: 0.97300\n(!!!) [EPOCH 39] Learning rate reduced from 0.02500 to 0.00625\n--- Epoch 40/300 ---\n  Train loss: 0.00170, train acc: 1.00000\n  Val   loss: 0.15247, val   acc: 0.97383\n--- Epoch 50/300 ---\n  Train loss: 0.00165, train acc: 1.00000\n  Val   loss: 0.15265, val   acc: 0.97383\n(!!!) [EPOCH 55] Learning rate reduced from 0.00625 to 0.00156\n--- Epoch 60/300 ---\n  Train loss: 0.00161, train acc: 1.00000\n  Val   loss: 0.15292, val   acc: 0.97383\n--- Epoch 70/300 ---\n  Train loss: 0.00160, train acc: 1.00000\n  Val   loss: 0.15299, val   acc: 0.97383\n(!!!) [EPOCH 71] Learning rate reduced from 0.00156 to 0.00039\n(!!!) Early stop! No validation accuracy improvement for 30 epochs!\nTRAINING: COMPLETE!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"predictions = model.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:15:26.771101Z","iopub.execute_input":"2025-11-11T08:15:26.771373Z","iopub.status.idle":"2025-11-11T08:15:26.834372Z","shell.execute_reply.started":"2025-11-11T08:15:26.771340Z","shell.execute_reply":"2025-11-11T08:15:26.833373Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"predictions_csv = {\n    \"ID\": list(range(len(predictions))),\n    \"target\": predictions,\n}\n\ndf = pd.DataFrame(predictions_csv)\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:15:26.835238Z","iopub.execute_input":"2025-11-11T08:15:26.835489Z","iopub.status.idle":"2025-11-11T08:15:26.861097Z","shell.execute_reply.started":"2025-11-11T08:15:26.835463Z","shell.execute_reply":"2025-11-11T08:15:26.860122Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"end_time = time.time()\ntotal_time = end_time - start_time\nprint(f\"TOTAL RUNTIME: {total_time:.2f} sec ({int(total_time/60)} min and {(total_time - int(total_time/60) * 60)} sec)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:15:26.861966Z","iopub.execute_input":"2025-11-11T08:15:26.862205Z","iopub.status.idle":"2025-11-11T08:15:26.867138Z","shell.execute_reply.started":"2025-11-11T08:15:26.862184Z","shell.execute_reply":"2025-11-11T08:15:26.866295Z"}},"outputs":[{"name":"stdout","text":"TOTAL RUNTIME: 73.83 sec (1 min and 13.834747314453125 sec)\n","output_type":"stream"}],"execution_count":11}]}