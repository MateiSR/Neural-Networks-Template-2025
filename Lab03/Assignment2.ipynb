{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4687354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ae562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./kaggle/input/fii-nn-2025-homework-2/extended_mnist_train.pkl\"\n",
    "test_file = \"./kaggle/input/fii-nn-2025-homework-2/extended_mnist_test.pkl\"\n",
    "\n",
    "with open(train_file, \"rb\") as fp:\n",
    "    train = pickle.load(fp)\n",
    "\n",
    "with open(test_file, \"rb\") as fp:\n",
    "    test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e26e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for image, label in train:\n",
    "    train_data.append(image.flatten())\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b718357",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for image, label in test:\n",
    "    test_data.append(image.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numpy_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X_train = np.array(train_data)\n",
    "y_train = np.array(train_labels)\n",
    "X_test = np.array(test_data)\n",
    "\n",
    "# normalize data (255 values for 0-255 black-white images)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    # one-hot, meaning:\n",
    "    # for digit n, we create a vector of size num_classes\n",
    "    # all values are 0, except the nth position, which is 1\n",
    "    one_hot_y = np.zeros((y.shape[0], num_classes))\n",
    "    for row, col in enumerate(y):\n",
    "        one_hot_y[row, col] = 1 # row = sample, col = digit 0-9\n",
    "    return one_hot_y\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate=0.1):\n",
    "        # weight initialized with small random values\n",
    "        self.W = np.random.randn(n_inputs, n_outputs) * 0.01\n",
    "        # bias initialized with small random values\n",
    "        self.b = np.random.randn(n_outputs) * 0.01\n",
    "        # learning rate\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        # softmax activation formula\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        ### forward step\n",
    "        # @ = matrix multiplication\n",
    "        # z = XW + b\n",
    "        z = X @ self.W + self.b\n",
    "        # apply softmax activation\n",
    "        y = self._softmax(z)\n",
    "        return y\n",
    "\n",
    "    def fit(self, X, y, epochs=150):\n",
    "        # number of samples\n",
    "        num_samples = X.shape[0]\n",
    "        num_classes = len(np.unique(y)) # always 10 (digits 0 to 9)\n",
    "        y_one_hot = one_hot(y, num_classes)\n",
    "        print(\"TRAINING: START!\")\n",
    "        # pass through the dataset EPOCHS times\n",
    "        for epoch in range(epochs):\n",
    "            # forward pass, predicted probabilities\n",
    "            y_predicted_probabilities = self.forward(X)\n",
    "            # y_one_hot is the expected output (Target)\n",
    "            # y_predicted_probabilities is the predicted output (y)\n",
    "            error_term = y_one_hot - y_predicted_probabilities # Target - y\n",
    "            grad_W = X.T @ error_term # (Target - y) * Tranpose(X)\n",
    "            grad_b = np.sum(error_term, axis=0) # sum of errors (Target - y)\n",
    "\n",
    "            # learning rate is the percent of the \"correction\" we apply to W and b\n",
    "            self.W += self.lr * (grad_W / num_samples) # W = W + lr * gradient_W\n",
    "            self.b += self.lr * (grad_b / num_samples) # b = b + lr * gradient_b\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                # cross-entropy loss\n",
    "                loss = -np.sum(y_one_hot * np.log(y_predicted_probabilities)) / num_samples\n",
    "                print(f\"EPOCH {epoch + 1}/{epochs} WITH LOSS: {loss:.5f}\")\n",
    "        print(\"TRAINING: COMPLETE!\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # get predicted probabilities (returns a matrix of 10 columns,\n",
    "        # one for each digit 0-9) and len(X) rows\n",
    "        y_probabilities = self.forward(X)\n",
    "        # return the label with highest probability for each sample\n",
    "        return np.argmax(y_probabilities, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41796d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: START!\n",
      "EPOCH 10/300 WITH LOSS: 0.80810\n",
      "EPOCH 20/300 WITH LOSS: 0.55922\n",
      "EPOCH 30/300 WITH LOSS: 0.46085\n",
      "EPOCH 40/300 WITH LOSS: 0.43039\n",
      "EPOCH 50/300 WITH LOSS: 0.41013\n",
      "EPOCH 60/300 WITH LOSS: 0.39517\n",
      "EPOCH 70/300 WITH LOSS: 0.38353\n",
      "EPOCH 80/300 WITH LOSS: 0.37414\n",
      "EPOCH 90/300 WITH LOSS: 0.36635\n",
      "EPOCH 100/300 WITH LOSS: 0.35975\n",
      "EPOCH 110/300 WITH LOSS: 0.35405\n",
      "EPOCH 120/300 WITH LOSS: 0.34908\n",
      "EPOCH 130/300 WITH LOSS: 0.34467\n",
      "EPOCH 140/300 WITH LOSS: 0.34074\n",
      "EPOCH 150/300 WITH LOSS: 0.33721\n",
      "EPOCH 160/300 WITH LOSS: 0.33400\n",
      "EPOCH 170/300 WITH LOSS: 0.33107\n",
      "EPOCH 180/300 WITH LOSS: 0.32838\n",
      "EPOCH 190/300 WITH LOSS: 0.32591\n",
      "EPOCH 200/300 WITH LOSS: 0.32361\n",
      "EPOCH 210/300 WITH LOSS: 0.32148\n",
      "EPOCH 220/300 WITH LOSS: 0.31949\n",
      "EPOCH 230/300 WITH LOSS: 0.31763\n",
      "EPOCH 240/300 WITH LOSS: 0.31589\n",
      "EPOCH 250/300 WITH LOSS: 0.31424\n",
      "EPOCH 260/300 WITH LOSS: 0.31269\n",
      "EPOCH 270/300 WITH LOSS: 0.31123\n",
      "EPOCH 280/300 WITH LOSS: 0.30984\n",
      "EPOCH 290/300 WITH LOSS: 0.30852\n",
      "EPOCH 300/300 WITH LOSS: 0.30726\n",
      "TRAINING: COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 784 # 28x28 images flattened\n",
    "n_outputs = 10 # digits 0-9\n",
    "learning_rate = 0.8\n",
    "epochs = 300\n",
    "\n",
    "model = Perceptron(n_inputs=n_inputs, n_outputs=n_outputs, learning_rate=learning_rate)\n",
    "model.fit(X_train, y_train, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3570a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d3ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "predictions_csv = {\n",
    "    \"ID\": list(range(len(predictions))),\n",
    "    \"target\": predictions,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(predictions_csv)\n",
    "df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
